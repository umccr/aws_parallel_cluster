[aws]
aws_region_name = ap-southeast-2

[global]
cluster_template = tothill 
update_check     = true
sanity_check     = true

## Clusters
[cluster tothill]
base_os               = alinux2
key_name              = romanvg 
vpc_settings          = tothill_network
efs_settings          = awselasticfs
s3_read_resource      = *
master_instance_type  = t2.medium
# Until the SLURM_COMPUTE_NODE_REAL_MEM is part of a ssm-parameter this will
# compute_instance_type will need to be uniform across each cluster template
additional_iam_policies = arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
scheduler             = slurm
pre_install           = s3://tothill-parallel-cluster-dev/__VERSION__/bootstrap/pre_install.sh
post_install          = s3://tothill-parallel-cluster-dev/__VERSION__/bootstrap/post_install.sh
custom_ami            = ami-074416eece29e32ec
master_root_volume_size  = 45  # Gb - for multiple conda installations
compute_root_volume_size = 60  # Gb - for multiple docker containers to be pulled down to run commands
# Set the queue
queue_settings        = compute, compute-long, copy, long


[cluster umccr_dev]
base_os               = alinux2
vpc_settings          = umccr_dev_network
efs_settings          = awselasticfs
s3_read_resource      = *
key_name              = pc-default-key
# Need something substantial to hold the slurm database
master_instance_type  = t2.medium
# Using additional_iam_policies over ec2_iam_role
# Add SSM policy, so we can keep port 22 closed
additional_iam_policies = arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
scheduler             = slurm
# Basic ami with installations of
# R, python3.8, conda, pip, ruby, golang, rust
# conda has been initialised to the ec2-user
# See the ami readme for recreation of the custom_ami
custom_ami            = ami-0cf68ccca246d4f2f
pre_install           = s3://umccr-research-dev/parallel-cluster/__VERSION__/bootstrap/pre_install.sh
post_install          = s3://umccr-research-dev/parallel-cluster/__VERSION__/bootstrap/post_install.sh
master_root_volume_size  = 45  # Gb - for multiple conda installations
compute_root_volume_size = 60  # Gb - for multiple docker containers to be pulled down to run commands
# Set the queue(s)
queue_settings        = compute, compute-long, copy, long

[cluster umccr_dev_fsx]
base_os               = alinux2
vpc_settings          = umccr_dev_network
fsx_settings          = lustrefs
s3_read_resource      = *
key_name              = pc-default-key
# Need something substantial to hold the slurm database
master_instance_type  = t2.medium
# Using additional_iam_policies over ec2_iam_role
# Add SSM policy, so we can keep port 22 closed
additional_iam_policies = arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
scheduler             = slurm
# Basic ami with installations of
# R, python3.8, conda, pip, ruby, golang, rust
# conda has been initialised to the ec2-user
# See the ami readme for recreation of the custom_ami
custom_ami            = ami-0cf68ccca246d4f2f
pre_install           = s3://umccr-research-dev/parallel-cluster/__VERSION__/bootstrap/pre_install.sh
post_install          = s3://umccr-research-dev/parallel-cluster/__VERSION__/bootstrap/post_install.sh
master_root_volume_size = 45   # Gb - for multiple conda installations
compute_root_volume_size = 60  # Gb - for multiple docker containers to be pulled down to run commands
# Set the queue(s)
queue_settings        = compute, compute-long, copy, long

## Networks
[vpc tothill_network]
vpc_id = vpc-06daaa8ca8e5c853e
master_subnet_id = 	subnet-061d824f3056967b5
use_public_ips = true
additional_sg  = sg-0b4296d2fae709454

[vpc umccr_dev_network]
# Default vpc for dev account
vpc_id = vpc-00eafc63c0dfca266
# Compute subnet ids take the same subnet as the master
# Our default public subnet
master_subnet_id = subnet-0fab038b0341872f1
# Elastic IP address is associated to the master instance.
use_public_ips = true
# sg-0ca5bdaab39885649: Security group where the slurm accounting database sits
additional_sg         = sg-0ca5bdaab39885649

[aliases]
ssh = ssh {CFN_USER}@{MASTER_IP} {ARGS}

## Filesystems
[efs awselasticfs]
shared_dir       = efs
encrypted        = true
performance_mode = generalPurpose

[fsx lustrefs]
# The mountpoint of the filesystem
shared_dir                = /fsx
# Storage available in GB
storage_capacity          = 1200
# Default system
deployment_type           = SCRATCH_2

## Resource partitions / queues
# The heavy lifting
[queue compute]
compute_resource_settings = hi_cpu, hi_mem
compute_type = spot

[queue compute-long]
compute_resource_settings = hi_cpu_long, hi_mem_long
compute_type = ondemand

# Staging data
[queue copy]
compute_resource_settings = hi_network
compute_type = spot

# For job schedulers that need reliability
# Non-spot instances
[queue long]
compute_resource_settings = general_purpose
compute_type = ondemand

## Compute resources
[compute_resource hi_cpu]
instance_type = c5.4xlarge  # 16 Cpus and 32 Gbs of RAM

[compute_resource hi_mem]
instance_type = m5.4xlarge	 # 16 Cpus and 128 Gbs of RAM

[compute_resource hi_cpu_long]
instance_type = c5.4xlarge  # 16 Cpus and 32 Gbs of RAM

[compute_resource hi_mem_long]
instance_type = m5.4xlarge	 # 16 Cpus and 128 Gbs of RAM

[compute_resource hi_network]
instance_type = m5.large    # 2 Cpus and 8 Gb of RAM

[compute_resource general_purpose]
instance_type = m5.large    # 2 CPUS and 8 Gbs of RAM

