on:
  push:
    # Sequence of patterns matched against refs/tags
    tags:
    - 'v*' # Push events to matching v*, i.e. v1.0, v20.15.10
    - 'pre-v*'  # Push events to matching pre-v, i.e pre-v1.0, pre-v20.15.10

name: Create and Upload Release

jobs:
  build:
    name: Build and Upload Release Assets
    runs-on: ubuntu-latest
    ## Only if we're on the main branch
    #if: github.event.base_ref == 'refs/heads/main'
    # Steps to run
    steps:
      # Standard checkout step
      - name: Checkout code
        uses: actions/checkout@v2
      # Get tag name, from /ref/heads/<tag> to <tag>
      - name: Get Tag Name
        id: get_tag_name
        run: echo "::set-output name=tag_name::$(basename ${{ github.ref }})"
      # Is pre-release?
      - name: Determine if this is a pre-release or not
        id: is_prerelease
        run: echo "::set-output name=prerelease::$(if [[ "${{ steps.get_tag_name.outputs.tag_name }}" == "pre"* || "${{ steps.get_tag_name.outputs.tag_name }}" == "dev" ]]; then echo true; else echo false; fi)"
      # Setup aws credentials so we can sync s3 files (UMCCR_DEV)
      - name: Configure AWS Credentials UMCCR DEV
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.UMCCR_DEV_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.UMCCR_DEV_AWS_SECRET_ACCESS_KEY }}
          aws-region: ap-southeast-2
          role-to-assume: ${{ secrets.UMCCR_DEV_AWS_ROLE_TO_ASSUME }}
          role-external-id: github_actions_user_umccr_aws_parallel_cluster
          role-duration-seconds: 1200
          #role-session-name: parallel_cluster
      # Copy files to s3
      - name: Copy S3 files UMCCR DEV
        run: |
          # Install requirements
          apt-get update
          apt-get install -y \
            awscli \
            jq \
            rsync
          # Get config root with ssm-parameter key
          # TODO place under actions as its own custom runner
          S3_BUCKET_DIR_SSM_KEY="/parallel_cluster/main/s3_config_root"  # TODO
          s3_config_root="$(aws ssm get-parameter --name "${S3_BUCKET_DIR_SSM_KEY}" | {
                            jq --raw-output '.Parameter.Value'
                          })"
          # Now extend folder with the version/tag
          s3_config_version="${s3_config_root}/${{ steps.get_tag_name.outputs.tag_name }}"
          # Initialise folder ready for upload
          mkdir s3_sync_files
          # Add instance configuration files
          sed "s/__VERSION__/${{ steps.get_tag_name.outputs.tag_name }}/g" bootstrap/pre_install.sh > s3_sync_files/pre_install.sh
          sed "s/__VERSION__/${{ steps.get_tag_name.outputs.tag_name }}/g" bootstrap/post_install.sh > s3_sync_files/post_install.sh
          # Add folders to upload to s3
          rsync --archive slurm/ s3_sync_files/slurm/
          rsync --archive toil/ s3_sync_files/toil/
          rsync --archive cromwell/ s3_sync_files/cromwell/
          rsync --archive bcbio/ s3_sync_files/bcbio/
          # Now run sync command
          aws s3 sync s3_sync_files/ "${s3_config_version}/"
          # Delete sync folder
          rm -rf s3_sync_files/
      # Setup aws credentials so we can sync s3 files (tothill)
      #- name: Configure AWS Credentials Tothill
      #  uses: aws-actions/configure-aws-credentials@v1
      #  with:
      #    aws-access-key-id: ${{ secrets.TOTHILL_AWS_ACCESS_KEY_ID }}
      #    aws-secret-access-key: ${{ secrets.TOTHILL_AWS_SECRET_ACCESS_KEY }}
      #    aws-region: ap-southeast-2
      ## Copy files to s3
      #- name: Copy S3 files Tothill
      #  run: |
      #    # Get config root with ssm-parameter key
      #    S3_BUCKET_DIR_SSM_KEY="/parallel_cluster/main/s3_config_root"
      #    s3_config_root="$(aws ssm get-parameter --name "${S3_BUCKET_DIR_SSM_KEY}" | {
      #                      jq --raw-output '.Parameter.Value'
      #                    })"
      #    # Now extend folder with the version/tag
      #    s3_config_version="${s3_config_root}/${{ steps.get_tag_name.outputs.tag_name }}"
      #    # Initialise folder ready for upload
      #    mkdir s3_sync_files
      #    # Add instance configuration files
      #    sed "s/__VERSION__/${{ steps.get_tag_name.outputs.tag_name }}/g" bootstrap/pre_install.sh > s3_sync_files/pre_install.sh
      #    sed "s/__VERSION__/${{ steps.get_tag_name.outputs.tag_name }}/g" bootstrap/post_install.sh > s3_sync_files/post_install.sh
      #    # Add folders to upload to s3
      #    rsync --archive slurm/ s3_sync_files/slurm/
      #    rsync --archive toil/ s3_sync_files/toil/
      #    rsync --archive cromwell/ s3_sync_files/cromwell/
      #    rsync --archive bcbio/ s3_sync_files/bcbio/
      #    # Now run sync command
      #    aws s3 sync s3_sync_files/ "${s3_config_version}/"
      #    # Delete sync folder
      #    rm -rf s3_sync_files/
      # Build project
      - name: Build project # This would actually build your project, using zip for an example artifact
        run: |
          # Install commands
          apt-get update -y
          apt-get install -y \
            rsync \
            zip
          # Create release dir
          mkdir release-${{ steps.get_tag_name.outputs.tag_name }}
          # Update version template for config and for
          # pre-install and post-install scripts
          sed "s/__VERSION__/${{ steps.get_tag_name.outputs.tag_name }}/g" conf/pcluster.conf > release-${{ steps.get_tag_name.outputs.tag_name }}/config.yaml
          # Add scripts
          rsync --archive bin/ release-${{ steps.get_tag_name.outputs.tag_name }}/bin/
          # Add conda-env yaml
          cp conf/pcluster-env.yaml release-${{ steps.get_tag_name.outputs.tag_name }}/pcluster.env.yaml
          # Make install.sh an executable
          chmod +x install.sh
          # Add installation script
          mv install.sh release-${{ steps.get_tag_name.outputs.tag_name }}/install.sh
          # Create zip file
          zip -r release-${{ steps.get_tag_name.outputs.tag_name }}.zip release-${{ steps.get_tag_name.outputs.tag_name }}
          ls -lsrth .
      # Create release zip file
      - name: Create Release
        id: create_release
        uses: actions/create-release@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          tag_name: ${{ steps.get_tag_name.outputs.tag_name }}
          release_name: release-${{ steps.get_tag_name.outputs.tag_name }}
          draft: false
          prerelease: ${{ steps.is_prerelease.outputs.prerelease == "true" }}
      # Upload zip file as an asset
      - name: Upload Release Asset
        id: upload-release-asset
        uses: actions/upload-release-asset@v1
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          upload_url: ${{ steps.create_release.outputs.upload_url }} # This pulls from the CREATE RELEASE step above, referencing it's ID to get its outputs object, which include a `upload_url`. See this blog post for more info: https://jasonet.co/posts/new-features-of-github-actions/#passing-data-to-future-steps
          asset_path: release-${{ steps.get_tag_name.outputs.tag_name }}.zip
          asset_name: release-${{ steps.get_tag_name.outputs.tag_name }}.zip
          asset_content_type: application/zip